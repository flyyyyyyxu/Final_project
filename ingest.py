import os
import json
import pickle
import numpy as np
import pandas as pd
from tqdm import tqdm
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv
from collections import Counter

load_dotenv()
QIANFAN_API_KEY = os.getenv("QIANFAN_API_KEY")
qianfan_client = OpenAI(
    api_key=QIANFAN_API_KEY,
    base_url="https://qianfan.baidubce.com/v2",
)

TAG_MODEL = "ernie-speed-8k"  # ä½ å¯æ ¹æ®è´¦å·æƒ…å†µæ¢æˆæ›´ç¨³çš„æ¨¡å‹ï¼Œå¦‚ ernie-4.0-8k

DATA_DIR = "./data"
VECTOR_DIR = "./vector_store"
os.makedirs(VECTOR_DIR, exist_ok=True)

# -----------------------
# æŠ½å–åŸå¸‚æ°›å›´å…³é”®è¯ï¼ˆvibesï¼‰
# -----------------------


def extract_city_vibes(text: str, city: str) -> list[str]:
    """
    ç”¨åƒå¸†æ¨¡å‹ä»ä¸€æ¡æ¸¸è®°ï¼ˆæ ‡é¢˜+æ­£æ–‡ï¼‰ä¸­æŠ½å–åŸå¸‚æ°›å›´/ç‰¹ç‚¹å…³é”®è¯ã€‚
    è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¾‹å¦‚ ["æµªæ¼«", "é€‚åˆæ­¥è¡Œ", "å¤œæ™¯å¥½çœ‹"]ã€‚
    """
    # æˆªæ–­ä¸€ä¸‹ï¼Œé¿å…å¤ªé•¿
    short_text = text[:800]

    prompt = f"""
ä¸‹é¢æ˜¯ä¸€æ®µå…³äºåŸå¸‚ã€Œ{city}ã€çš„æ—…è¡Œæ¸¸è®°å†…å®¹ï¼Œè¯·ä½ ç”¨ 3-7 ä¸ªä¸­æ–‡å…³é”®è¯æ¦‚æ‹¬è¿™åº§åŸå¸‚åœ¨è¿™ç¯‡æ¸¸è®°ä¸­å‘ˆç°å‡ºæ¥çš„æ°›å›´å’Œç‰¹ç‚¹ã€‚
å…³é”®è¯å¯ä»¥æ˜¯ï¼šæƒ…ç»ªï¼ˆä¾‹å¦‚â€œæ”¾æ¾â€ã€â€œæµªæ¼«â€ã€â€œåˆºæ¿€â€ï¼‰ã€èŠ‚å¥ï¼ˆä¾‹å¦‚â€œæ­¥è¡Œå‹å¥½â€ã€â€œèŠ‚å¥å¾ˆå¿«â€ï¼‰ã€æ¶ˆè´¹æ„Ÿå—ï¼ˆä¾‹å¦‚â€œç‰©ä»·ä¾¿å®œâ€ã€â€œæ¯”è¾ƒè´µâ€ï¼‰ã€é€‚åˆäººç¾¤ï¼ˆä¾‹å¦‚â€œé€‚åˆæƒ…ä¾£â€ã€â€œé€‚åˆäº²å­â€ï¼‰ã€ç¯å¢ƒç‰¹ç‚¹ï¼ˆä¾‹å¦‚â€œå¤œæ™¯å¥½çœ‹â€ã€â€œè¡—åŒºå¾ˆæ–‡è‰ºâ€ç­‰ï¼‰ã€‚

ã€æ¸¸è®°å†…å®¹ã€‘
{short_text}

ã€è¾“å‡ºè¦æ±‚ã€‘
1. åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚
2. JSON æ•°ç»„å…ƒç´ æ˜¯ç®€çŸ­çš„ä¸­æ–‡çŸ­è¯­ï¼Œä¾‹å¦‚ï¼š
   ["æµªæ¼«", "é€‚åˆæ­¥è¡Œ", "ç¾é£Ÿä¸°å¯Œ", "å¤œæ™¯å¥½çœ‹", "ç‰©ä»·ç•¥è´µ"]
"""

    try:
        resp = qianfan_client.chat.completions.create(
            model=TAG_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æç‚¼åŸå¸‚æ—…è¡Œæ°›å›´å…³é”®è¯çš„åŠ©æ‰‹ã€‚",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            max_tokens=200,
        )

        content = resp.choices[0].message["content"]
        # å°è¯•æŒ‰ JSON è§£æ
        vibes = json.loads(content)
        if isinstance(vibes, list):
            cleaned = [v.strip() for v in vibes if isinstance(v, str) and v.strip()]
            return cleaned[:10]
        return []
    except Exception as e:
        print("extract_city_vibes error:", e)
        return []


# -----------------------
# Step 1: Load local embedding model
# -----------------------
print("Loading SentenceTransformer model...")
embedder = SentenceTransformer("all-MiniLM-L6-v2")
print("Model loaded.")


# -----------------------
# Step 2: Chunk function
# -----------------------
def chunk_text(text, max_tokens=200):
    words = str(text).split()
    chunks = []
    for i in range(0, len(words), max_tokens):
        chunks.append(" ".join(words[i : i + max_tokens]))
    return chunks


# -----------------------
# Step 3: Collect all CSVs
# -----------------------
def load_all_csv():
    medium_dir = os.path.join(DATA_DIR, "medium")
    reddit_dir = os.path.join(DATA_DIR, "reddit")

    medium_files = (
        [os.path.join(medium_dir, f) for f in os.listdir(medium_dir) if f.endswith(".csv")]
        if os.path.exists(medium_dir)
        else []
    )
    reddit_files = (
        [os.path.join(reddit_dir, f) for f in os.listdir(reddit_dir) if f.endswith(".csv")]
        if os.path.exists(reddit_dir)
        else []
    )

    print(f"Found Medium CSVs: {len(medium_files)}")
    print(f"Found Reddit CSVs: {len(reddit_files)}")

    return medium_files + reddit_files


# -----------------------
# Step 4: Chunk the CSV content
# -----------------------
def infer_city_from_path(csv_path: str) -> str:
    # ç¤ºä¾‹ï¼šdata/medium/paris_medium_posts.csv -> paris
    base = os.path.basename(csv_path).lower()
    # ä½ å¯ä»¥æŒ‰éœ€è¦è‡ªå·±å¢å‡åŸå¸‚å
    for token in ["paris", "budapest", "rome", "london", "tokyo", "kyoto"]:
        if token in base:
            return token
    return ""


def build_chunks(csv_files):
    chunks = []
    metadata = []

    for csv_path in csv_files:
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            print(f"[ERROR] Cannot read {csv_path}: {e}")
            continue

        # çŒœå­—æ®µå
        title_col = None
        url_col = None
        text_col = None

        for c in df.columns:
            lc = c.lower()
            if lc == "title":
                title_col = c
            if lc == "url":
                url_col = c
            if lc == "content":  # Medium
                text_col = c
            if lc == "selftext":  # Reddit
                text_col = c

        # å†å…œåº•ï¼šå–ç¬¬ 2 åˆ—å½“æ­£æ–‡
        if text_col is None and len(df.columns) >= 2:
            text_col = df.columns[1]

        city = infer_city_from_path(csv_path)

        print(f"Processing {csv_path} (city={city or 'æœªçŸ¥'}) rows={len(df)}")

        for i, row in df.iterrows():
            raw_text = str(row.get(text_col, ""))
            if not raw_text.strip():
                continue

            title = str(row.get(title_col, "")) if title_col else ""
            url = str(row.get(url_col, "")) if url_col else ""

            # âš ï¸ ç”¨â€œæ ‡é¢˜ + æ­£æ–‡å¼€å¤´â€ä½œä¸ºæ ‡ç­¾è¾“å…¥
            tag_input = (title + "\n" + raw_text).strip()
            vibes = extract_city_vibes(tag_input, city or "è¿™åº§åŸå¸‚")

            # å¯¹æ­£æ–‡åšåˆ†å—ï¼Œæ¯ä¸ª chunk å…±ç”¨åŒä¸€ä»½ metadataï¼ˆåŒ…æ‹¬ vibesï¼‰
            for c in chunk_text(raw_text):
                chunks.append(c)
                metadata.append(
                    {
                        "source": csv_path,
                        "row": int(i),
                        "content": c,
                        "title": title,
                        "url": url,
                        "city": city,
                        "vibes": vibes,  # ğŸ‘ˆ æŠŠæ°›å›´æ ‡ç­¾å†™è¿› metadata
                    }
                )

    return chunks, metadata


# -----------------------
# Step 5: Vectorize using local model
# -----------------------
def vectorize_chunks(chunks):
    embeddings = []

    for c in tqdm(chunks, desc="Embedding chunks"):
        vec = embedder.encode(c)
        embeddings.append(vec)

    return np.array(embeddings).astype("float32")


# -----------------------
# Step 6: Save vector store
# -----------------------
def save_vector_store(embeddings, metadata, chunks):
    if embeddings.shape[0] == 0:
        print("âŒ ERROR: No embeddings generated. Cannot save vector store.")
        return

    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    faiss.write_index(index, f"{VECTOR_DIR}/index.faiss")

    with open(f"{VECTOR_DIR}/metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    with open(f"{VECTOR_DIR}/chunks.pkl", "wb") as f:
        pickle.dump(chunks, f)

    print("âœ… Vector store saved!")


# -----------------------
# Step 7: èšåˆæ‰€æœ‰åŸå¸‚çš„å…³é”®è¯ï¼Œå†™å…¥ city_vibes.json
# -----------------------
def build_city_vibes(metadata):
    """
    ä»æ‰€æœ‰ metadata ä¸­èšåˆæ¯ä¸ªåŸå¸‚çš„ vibes å…³é”®è¯ï¼Œç»Ÿè®¡é¢‘æ¬¡ï¼Œå†™å…¥ä¸€ä¸ªæ–‡ä»¶ï¼š
    VECTOR_DIR/city_vibes.json

    ç»“æ„ç¤ºä¾‹ï¼š
    {
      "paris": {
        "vibes": ["æµªæ¼«", "é€‚åˆæ­¥è¡Œ", "ç¾é£Ÿä¸°å¯Œ"],
        "counts": {"æµªæ¼«": 12, "é€‚åˆæ­¥è¡Œ": 8, ...}
      },
      "budapest": {
        "vibes": [...],
        "counts": {...}
      }
    }
    """
    city_counters = {}

    for md in metadata:
        city = (md.get("city") or "").strip().lower()
        if not city:
            continue

        vibes = md.get("vibes", [])
        if not isinstance(vibes, list):
            continue

        if city not in city_counters:
            city_counters[city] = Counter()

        for v in vibes:
            v = str(v).strip()
            if v:
                city_counters[city][v] += 1

    summary = {}
    for city, counter in city_counters.items():
        top_vibes = [w for w, _ in counter.most_common(15)]
        summary[city] = {
            "vibes": top_vibes,
            "counts": dict(counter),
        }

    out_path = os.path.join(VECTOR_DIR, "city_vibes.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"âœ… city_vibes.json saved to {out_path}")


# -----------------------
# Main
# -----------------------
if __name__ == "__main__":
    csv_files = load_all_csv()
    chunks, metadata = build_chunks(csv_files)

    # å…ˆæ„å»ºåŸå¸‚å…³é”®è¯æ€»è¡¨
    build_city_vibes(metadata)

    # å†å‘é‡åŒ–å¹¶ä¿å­˜å‘é‡åº“
    embeddings = vectorize_chunks(chunks)
    print("Embeddings shape:", embeddings.shape)
    save_vector_store(embeddings, metadata, chunks)